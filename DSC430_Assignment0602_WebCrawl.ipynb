{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae74da9",
   "metadata": {},
   "source": [
    "## JITEN MISHRA DSC430_Assignment0602_WebCrawl\n",
    "### I have not given or received any unauthorized assistance on this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133a9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and import statements first\n",
    "from urllib.request import urlopen, urljoin, urlparse, Request\n",
    "from html.parser import HTMLParser\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40654a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visited = set() # initialize visited to an empty set\n",
    "freqdict = {}   # initialize the dictionary to hold frequency\n",
    "\n",
    "class Collector(HTMLParser):\n",
    "    \"\"\"\n",
    "    Collects hyperlink URLs into a list\n",
    "    Collects the data in a list from the url page\n",
    "    \"\"\"\n",
    "\n",
    "    ## ignore several tags and extentions and exclusions that helps\n",
    "    ## extraction of clean text and avoid error.  \n",
    "    # We define them as a class variable.\n",
    "    ignore_tags = ['script', 'noscript', 'input', 'meta', 'title', \n",
    "                   'style', 'form','img']\n",
    "    img_ext = ['.jpg','.png','.gif','.pdf','.zip']\n",
    "    exclude = ['action=download', 'mailto', 'course-evaluation', ' ','img']\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Initializes parser, the url, list for link and data content\n",
    "        Initializes the current tag\n",
    "        Fetches the stop words to be ignored \n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize the super class\n",
    "        HTMLParser.__init__(self)\n",
    "        self.url = url\n",
    "        self.links = []\n",
    "        \n",
    "        # variable to store the required data\n",
    "        self.dataContent = []\n",
    "        \n",
    "        # variable to store current tags\n",
    "        self.currentTag = ''\n",
    "        \n",
    "        # get the stop words from the file and close \n",
    "        stopWordFile = open('M6_stopwords.txt','r')\n",
    "        self.lsStopWords = stopWordFile.read().splitlines()\n",
    "        stopWordFile.close()\n",
    "        \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        \"\"\" \n",
    "        Collects hyperlink URLs in their absolute format \n",
    "        \"\"\"      \n",
    "        \n",
    "        # save the tag to the instance variable\n",
    "        self.currentTag = tag\n",
    "        excFlag = False\n",
    "        \n",
    "        # if the tag is a anchor tag, get the href links\n",
    "        # avoiding downaload actions and facweb server data\n",
    "        # append it to the instance variable list by converting\n",
    "        # it to a absolute url\n",
    "        # appending only links of cdm domain\n",
    "        # to optimize list data structure we check for global \n",
    "        # variable visited set() before adding the link to the list\n",
    "        # we are also excluding few file extention cdm links declared \n",
    "        # in class variable img_ext.\n",
    "        if tag == 'a':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'href':\n",
    "                    # represent the flag to exclude the link or not\n",
    "                    # if its in the exclusion list then exclude it\n",
    "                    for exc in self.exclude:\n",
    "                        if exc in attr[1].lower():\n",
    "                            excFlag = True\n",
    "                    \n",
    "                    # if the flag is not flase\n",
    "                    if(not excFlag):\n",
    "                        # convert the url to a absolute url\n",
    "                        absolute = urljoin(self.url, attr[1].lstrip())\n",
    "                        \n",
    "                        # proceed if its only a http url and the url is not a file extention\n",
    "                        # capture only cdm url.\n",
    "                        if absolute[:4] == 'http' and absolute[-4:] not in self.img_ext : # collect HTTP URL\n",
    "                            if('http://www.cdm.depaul.edu/' == absolute[:26] and absolute not in visited ):\n",
    "                                self.links.append(absolute)                            \n",
    "                        \n",
    "    def handle_data(self, data):\n",
    "        \"\"\"\n",
    "        Function to handle the data and create a list of data for the page\n",
    "        \"\"\"\n",
    "        \n",
    "        ## check the current tag.  If it's not in the \n",
    "        ## ignore_tags list, proceed to process the data.\n",
    "        if(self.currentTag not in self.ignore_tags):\n",
    "            # removes all the punctuations from the data and convert to lower case\n",
    "            # using the string punctuation data - !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "            data = data.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "            \n",
    "            # get the list of words\n",
    "            dataWord = data.split()\n",
    "            \n",
    "            # create an empty list data\n",
    "            lsData = []\n",
    "            \n",
    "            # ignore the words that are present in the \n",
    "            # stop words list and get only alphabatic words\n",
    "            for word in dataWord:\n",
    "                if(word not in self.lsStopWords and word.isalpha()):\n",
    "                    lsData.append(word)\n",
    "            \n",
    "            # accumulate the data to the instance variable \n",
    "            if(len(lsData)>0):\n",
    "                self.dataContent.append(' '.join(lsData))\n",
    "            \n",
    "    def getLinks(self):\n",
    "        \"\"\" \n",
    "        Returns hyperlinks URLs in their absolute format \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.links\n",
    "\n",
    "    \n",
    "    def getData(self):\n",
    "        \"\"\" \n",
    "        Returns the data (accumulated in the instance variable) \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.dataContent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38cc3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl2(url):\n",
    "    \"\"\" \n",
    "    A recursive web crawler that calls analyze()\n",
    "    on every visited web page \n",
    "    \"\"\"\n",
    "    \n",
    "    # global variables warning\n",
    "    global visited\n",
    "    \n",
    "    # add url to set of visited pages\n",
    "    visited.add(url)\n",
    "\n",
    "    # analyze() returns a list of hyperlink URLs in web page url \n",
    "    links = analyze(url)\n",
    "              \n",
    "    # recursively continue crawl from every link in links\n",
    "    for link in links:\n",
    "        # follow link only if not visited\n",
    "        if link not in visited:\n",
    "            try:\n",
    "                crawl2(link)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a40f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(url):\n",
    "    \"\"\"\n",
    "    Function to analyze the url and capture frequency of words\n",
    "    to the global directory\n",
    "    \"\"\"\n",
    "    \n",
    "    # set request header to avoid the HTTP error 418\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 6.1; Win64; x64)\"\n",
    "    request = Request(url)\n",
    "    request.add_header(\"User-Agent\",user_agent)    \n",
    "    content = urlopen(request).read().decode('utf8', errors='ignore')\n",
    "    \n",
    "    # obtain links in the web page\n",
    "    collector = Collector(url)\n",
    "    collector.feed(content)\n",
    "    urls = collector.getLinks()          # get list of links\n",
    "\n",
    "    # compute word frequencies\n",
    "    content = collector.getData()\n",
    "    frequency(content)\n",
    "    \n",
    "    # return the urls\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b380ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(sents):\n",
    "    \"\"\"\n",
    "    Function to capture the word with frequencies in the dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    #global variable warning\n",
    "    global freqdict\n",
    "    \n",
    "    # loop through the content and add word and frequency\n",
    "    # to the dictionary\n",
    "    for sent in sents:\n",
    "        words = sent.split()\n",
    "        for word in words:\n",
    "            if word in freqdict:\n",
    "                freqdict[word] += 1\n",
    "            else:\n",
    "                freqdict[word] = 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "720e8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeOutput():\n",
    "    \"\"\"\n",
    "    Function to write the data to the output file\n",
    "    \"\"\"\n",
    "    \n",
    "    # open a file to write data in desired format\n",
    "    file = open('crawl1.txt','w') \n",
    "    file.write(\"Total Number of page crawled:::: {}\\n\".format(len(visited)))\n",
    "    file.write(\"The 50 most common words and their frequencies are::::\\n\")\n",
    "    \n",
    "    # initialise sl to write serial numbers to the file\n",
    "    sl = 0\n",
    "    \n",
    "    # sort the dictionary based on the value in reverse\n",
    "    # and get the first 50\n",
    "    freqdictSorted = dict(sorted(freqdict.items(), key=lambda x:x[1],reverse=True)[:50])\n",
    "    \n",
    "    # loop through the sorted dictionary\n",
    "    # and write the word and the frequency \n",
    "    # to the file with formatting\n",
    "    for key in freqdictSorted.keys():\n",
    "        sl +=1\n",
    "        wrd = str(key)[:15]                   \n",
    "        count = str(freqdictSorted[key])\n",
    "        file.write(\"\\n{:5}: {:15} {:10}\".format(sl,wrd,count))\n",
    "        \n",
    "    # close the file\n",
    "    file.close()\n",
    "        \n",
    "    print(\"End of Code run\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626cf8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Web Crawling Started....\n",
      "\n",
      "Web Crawling Ended....\n",
      "\n",
      "Writting Output to file....\n",
      "End of Code run\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWeb Crawling Started....\")\n",
    "crawl2('http://www.cdm.depaul.edu')\n",
    "print(\"\\nWeb Crawling Ended....\")\n",
    "print(\"\\nWritting Output to file....\")\n",
    "writeOutput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12d7b1",
   "metadata": {},
   "source": [
    "#### Few Important points on Approach\n",
    "   * How You Extended HTML Parser ?\n",
    "       * The HTMLParser is extended in class Collector by initializing the object from the init method of Collecter class and overriding the methods to obtain desired output.\n",
    "       \n",
    "   * Which methods you overwrote ?\n",
    "       * Method : handle_starttag and handle_data are overwritten.\n",
    "       \n",
    "   * How you restricted your search to webpages at CDM ?\n",
    "       * In the handle_startag method we have a logic to capture the links in it absolute form when ever the tag is an anchor tag. While doing so we checke if the first 26 character of the absolute url is 'http://www.cdm.depaul.edu/' only then we add the link to be searched.\n",
    "       * We also have logic to avoid files of few extenstions stored in a list and we have also avoided download links with few other exclusions in order for the program to run smoothly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
